# ğŸ¤– AI Service Boilerplate 2025 â€” by Wali

Ce dÃ©pÃ´t est mon **boilerplate IA** prÃªt pour la prod : une **API FastAPI** compatible **OpenAI-like** (chat/embeddings), avec **Docker**, **tests**, **CI sÃ©curitÃ©** (ruff, mypy, pytest, gitleaks, trivy), **SBOM** (Syft) et **CD** optionnel vers AWS (rÃ©utilisable avec mon infra).

![Architecture](diagrams/architecture.png)

## âœ¨ FonctionnalitÃ©s
- API **/healthz**, **/v1/chat/completions** (format OpenAI), **/v1/embeddings**.
- Adapteurs **modÃ¨les** : **Hugging Face Transformers** (CPU par dÃ©faut), **Ollama** (local), et point dâ€™extension **vLLM**.
- **Rate limiting** & **API keys** (simple) + **CORS**.
- **Logs structurÃ©s** (JSON), **traces** OpenTelemetry (HTTP).
- **Tests** (pytest) & **type-checking** (mypy), **lint** (ruff).
- **Docker multi-Ã©tapes**; **SBOM** (Syft), scan **Trivy**, **gitleaks**.

## ğŸš€ DÃ©marrer en local
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

Test rapide :
```bash
curl -s http://localhost:8000/healthz
curl -s http://localhost:8000/v1/chat/completions -H 'Content-Type: application/json' -d '{"model":"hf:distilbert-base-uncased","messages":[{"role":"user","content":"Hello"}]}'
```

## ğŸ§ª QualitÃ©
```bash
ruff check .
mypy app
pytest -q
```

## ğŸ›¡ï¸ CI/CD
- **CI** : lint (ruff), mypy, tests (pytest), SBOM (syft), scans (gitleaks, trivy), build Docker.
- **CD** : prÃªt Ã  brancher sur ton infra AWS (ECR/ECS).

## ğŸ“¦ Build Docker
```bash
docker build -t wali/ai-service:dev .
docker run -p 8000:8000 -e API_KEYS='["changeme"]' wali/ai-service:dev
```

## ğŸ” Config sÃ©curitÃ© (env)
- `API_KEYS` : liste JSON de clÃ©s API autorisÃ©es (ex: `["devkey"]`).
- `ALLOW_ORIGINS` : CORS (ex: `["*"]` pour dev, restreint en prod).
- `MODEL_BACKEND` : `hf`, `ollama` ou `mock`.
- `HF_MODEL_ID` : ex. `distilbert-base-uncased` (dÃ©mo CPU).
- `OLLAMA_HOST` : ex. `http://localhost:11434`.

## ğŸ§© Routes
- `GET /healthz`
- `POST /v1/chat/completions` (OpenAI-like minimal)
- `POST /v1/embeddings`

## ğŸ—ºï¸ Roadmap
- vLLM backend, batching, queue SQS, cache Redis, monitoring Prometheus, limites de tokens, persistance S3.

---
Made by **Wali Diabi** â€” 2025
